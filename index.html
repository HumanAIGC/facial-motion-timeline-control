<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="FaceTimeLine">
    <meta property="og:title" content="FaceTimeLine"/>
    <meta property="og:description"
          content="Exploring Timeline Control for Facial Motion Generation."/>
    <meta property="og:url" content="URL OF THE WEBSITE"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/video_t1.png"/>
    <meta property="og:image:width" content="2412"/>
    <meta property="og:image:height" content="1394"/>


    <meta name="twitter:title" content="FaceTimeLine">
    <meta name="twitter:description"
          content="Exploring Timeline Control for Facial Motion Generation.">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/video_t1.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="Image-to-Video">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Exploring Timeline Control for Facial Motion Generation</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link href="https://fonts.googleapis.com/css2?family=Jost:wght@300;400;500&display=swap" rel="stylesheet">

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>

<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
    </div>
    <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
            <a class="navbar-item" href=https://github.com/HumanAIGC>
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
            </a>

            <div class="navbar-item has-dropdown is-hoverable">
                <a class="navbar-link">
                    More Research
                </a>
                <div class="navbar-dropdown">
                    <a class="navbar-item" href="https://humanaigc.github.io/emote-portrait-alive-2/">
                        EMO2
                    </a>
                    <a class="navbar-item" href="https://humanaigc.github.io/animate-anyone-2/">
                        AnimateAnyone2
                    </a>
                    <a class="navbar-item" href="https://humanaigc.github.io/chat-anyone/">
                        ChatAnyone
                    </a>
                    <a class="navbar-item" href="https://humanaigc.github.io/omnitalker/">
                        Omnitalker
                    </a>

                </div>
            </div>
        </div>

    </div>
</nav>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Exploring Timeline Control for Facial Motion Generation</h1>
                    <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <span class="author-block">
                <a href="" target="_blank">Yifeng Ma</a><sup>1</sup>,</span>
                        <span class="author-block">
                <a href="https://dblp.org/pid/183/0937.html" target="_blank">Jinwei Qi</a>,</span>
                        <span class="author-block">
                  <a href="https://dblp.org/pid/189/3461.html" target="_blank">Chaonan ji</a>,</span>
                        <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=QTgxKmkAAAAJ" target="_blank">Peng Zhang</a>,</span>
                        <span class="author-block">
                      <a href="https://dblp.org/pid/11/4046.html" target="_blank">Bang Zhang</a><sup>2</sup>,</span>
                        <span class="author-block">
                         <a href="" target="_blank">Zhidong Deng</a><sup>1</sup>,</span>
                            <span class="author-block">
                          <a href="https://scholar.google.com/citations?user=FJwtMf0AAAAJ&hl=zh-CN" target="_blank">Liefeng Bo</a>
                          <sup>2</sup></span>
                    </div>
                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>Tsinghua University,</span>
                        <span class="author-block"><sup>2</sup>Institute for Intelligent Computing, Alibaba Group</span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
<!--                <span class="link-block">-->
<!--                  <a href="https://github.com/HumanAIGC/EMO" target="_blank"-->
<!--                     class="external-link button is-normal is-rounded is-dark">-->
<!--                    <span class="icon">-->
<!--                      <i class="fab fa-github"></i>-->
<!--                    </span>-->
<!--                    <span>GitHub</span>-->
<!--                  </a>-->
<!--                </span>-->
                            <span class="link-block">
                  <a href="" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="hero is-small">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <!-- <h2 class="title is-3">Facial Motion Generation</h2> -->
            <div style="display: flex; justify-content: center; align-items: center;">
                <img src="content/v2/teaser.png" alt="MY ALT TEXT" style="width: 95%; height: 95%;"/>
            </div>
            <div class="item">
                <h2 class="content has-text-justified">
                    <p style="font-size: 1.2em;">
                        We introduce a new control signal for facial motion generation: timeline control. We first utilize a labor-efficient approach to annotate the time intervals of facial motion at a frame-level granularity. Using the annotations, we propose a model that can generate natural facial motions aligned with an input timeline. Compared to previous controls like audio and text, timeline control enables precise temporal control of facial motions. In this paper, facial motions are rendered into photorealistic videos for better visualization.
                    </p>
                </h2>
                <div class="item">
                </div>
            </div>
</section>



<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <!--                <div style="display: flex; justify-content: center; align-items: center;">-->
                <!--                    <img src="content/images/framework/intro.png" alt="MY ALT TEXT" style="width: 80%; height: 80%;"/>-->
                <!--                </div>-->
                <div class="content has-text-justified">
                    <p style="font-size: 1.2em;">
                        This paper introduces a new control signal for facial motion generation: timeline control. 
                        Compared to audio and text signals, timelines provide more fine-grained control, such as generating specific facial motions with precise timing.
                        Users can specify a multi-track timeline of facial actions arranged in temporal intervals, allowing precise control over the timing of each action.
                        To model the timeline control capability, We first annotate the time intervals of facial actions in natural facial motion sequences at a frame-level granularity. 
                        This process is facilitated by Toeplitz Inverse Covariance-based Clustering to minimize human labor.
                        Based on the annotations, we propose a diffusion-based generation model capable of generating facial motions that are natural and accurately aligned with input timelines.
                        Our method supports text-guided motion generation by using ChatGPT to convert text into timelines.
                        Experimental results show that our method can annotate facial action intervals with satisfactory accuracy, and produces natural facial motions accurately aligned with timelines.
                    </p>
                </div>
            </div>
        </div>
        <!--                                            <video class="video-player" poster="" id="tree1" controls>-->
        <!--                    <source src="content/video/main_page.mp4" type="video/mp4">-->
        <!--                </video>-->
    </div>
</section>

<!-- Method -->
<section class="hero is-small">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <h2 class="title is-2">Facial Motion Annotation</h2>
            <div style="display: flex; justify-content: center; align-items: center;">
                <img src="content/v2/annotation_pipeline.png" alt="MY ALT TEXT" style="width: 95%; height: 95%;"/>
            </div>
            <div class="item">
                <h2 class="content has-text-justified">
                    <p style="font-size: 1.2em;">
                        The pipeline of frame-level facial motion annotation (using brow motions as an example). We first extract facial motion descriptors (blendshapes)  from natural facial motion videos and concatenate the results to create a facial motion time series for time series analysis. This analysis can simultaneously segment the sequence into a series of motion patterns and cluster similar patterns, resulting in multiple clusters, each containing consistent facial motion patterns. Then, by inspecting a few patterns, we identify the facial motions each cluster represents, thereby obtaining frame-level facial motion annotations for all videos.
                    </p>
                </h2>
                <div class="item">
                </div>
            </div>
</section>

<section class="hero is-small">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <h2 class="title is-2">Facial Motion Generation</h2>
            <div style="display: flex; justify-content: center; align-items: center;">
                <img src="content/v2/generation_pipeline.png" alt="MY ALT TEXT" style="width: 95%; height: 95%;"/>
            </div>
            <div class="item">
                <h2 class="content has-text-justified">
                    <p style="font-size: 1.2em;">
                        Illustration of generation model. (a) Base-Branch Design.  The base network takes the timelines of all facial regions as input and outputs base features that model the global facial motion couplings. Through timeline selection, each region's timeline is directed to its respective branch network. Since head pose is interconnected with all facial movements, the pose branch receives timelines of all regions. Each branch network takes the timeline of its corresponding region to generate the facial motions for that region. These motions are then combined to produce the overall motion of the entire face.
  Lin. Proj. denotes Linear Projection.
  (b) Base/Branch Network's Architecture.
  Timeline control guides motion generation through cross-attention. The initial timeline tokens remain unchanged and are added at each layer.  The diffusion step (omitted in sub-figure (a) for clarity) is applied to each base and branch network.
                    </p>
                </h2>
                <div class="item">
                </div>
            </div>
</section>
<!-- End Method -->

<style>
    .gifImage:hover {
        opacity: 0.8;
        box-shadow: 0 0 5px rgba(0, 0, 0, 0.5);
        transform: scale(1.1);
    }

    .paused {
        animation-play-state: paused;
    }

</style>


<head>
    <title>place gif</title>
    <style>
        .gif-container {
            display: flex;
        }

        .gif {
            width: 660px; /* 设置 GIF 的宽度 */
            height: 400px; /* 设置 GIF 的高度 */
        }
    </style>
</head>

<section class="hero is-small is-light">
    <div class="hero-body">
        <div class="container">
            <h2 class="title is-2">Result</h2>
            <br></br>
<!--            <h3 class="title is-3">Singing</h3>-->
            <h2 class="content has-text-justified">
                <p style="font-size: 1.4em;"></p>
            </h2>

            <div style="display: flex; justify-content: center;align-items: center; height: 80px;">
                <h3 class="title is-4"> Facial Motion Generation from Timeline </h3>
            </div>

            <div class="video-item">
                <div class="column" style="width:100%;">
                    <video class="video-player" poster="" id="tree2" controls>
                        <source src="content/v2/timeline_gen_v2.mp4" type="video/mp4">
                    </video>
                </div>
            </div>

            <div style="display: flex; justify-content: center;align-items: center; height: 80px;">
                <h3 class="title is-4"> Video Edit on Timeline </h3>
            </div>

            <div class="video-item">
                <div class="column" style="width:100%;">
                    <video class="video-player" poster="" id="tree2" controls>
                        <source src="content/v2/timeline_edit.mp4" type="video/mp4">
                    </video>
                </div>
            </div>

            <div style="display: flex; justify-content: center;align-items: center; height: 80px;">
                <h3 class="title is-4"> Text Control Timeline Generation </h3>
            </div>

            <div class="video-item">
                <div class="column" style="width:100%;">
                    <video class="video-player" poster="" id="tree2" controls>
                        <source src="content/v2/textgen_timeline.mp4" type="video/mp4">
                    </video>
                </div>
            </div>

            <div style="display: flex; justify-content: center;align-items: center; height: 80px;">
                <h3 class="title is-4"> Timeline Annotation </h3>
            </div>

            <div class="video-item">
                <div class="column" style="width:100%;">
                    <video class="video-player" poster="" id="tree2" controls>
                        <source src="content/v2/timeline_anno.mp4" type="video/mp4">
                    </video>
                </div>
            </div>


            <br></br>
        </div>
    </div>
</section>


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content" style="text-align: center;">
                    <p>
                        This project is intended solely for academic research and effect demonstration.
                    </p>

                    <p>
                        This page was built using the <a
                            href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                            target="_blank">Template</a> which was adopted from the <a href="https://nerfies.github.io"
                                                                                       target="_blank">Nerfies</a> project
                        page.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

<script src="static/js/script.js"></script>
<script>
    // 获取所有视频元素
    var videos = document.querySelectorAll('video');

    // 为每个视频添加播放事件监听器
    videos.forEach(function (video) {
        video.addEventListener('play', function () {
            // 当任何一个视频开始播放时，暂停其他所有视频
            videos.forEach(function (otherVideo) {
                if (otherVideo !== video) {
                    otherVideo.pause();
                }
            });
        }, false);
    });
</script>

<script>
    new BeforeAfter({
        id: '#example1'
    });
    new BeforeAfter({
        id: '#example2'
    });
    new BeforeAfter({
        id: '#example3'
    });
    new BeforeAfter({
        id: '#example4'
    });
    new BeforeAfter({
        id: '#example6'
    });
    new BeforeAfter({
        id: '#example7'
    });

</script>

<script>
    var gifImage = document.getElementById('gifImage');
    var isPaused = false;

    gifImage.addEventListener('mouseenter', function () {
        gifImage.src = gifImage.src;
        isPaused = true;
    });

    gifImage.addEventListener('mouseleave', function () {
        if (isPaused) {
            gifImage.src = gifImage.src;
            isPaused = false;
        }
    });
</script>

<script>
    bulmaCarousel.attach('#results-carousel11', {
        slidesToScroll: 1,
        slidesToShow: 2,
        infinite: true,
        autoplay: false,
    });
    bulmaCarousel.attach('#results-carousel22', {
        slidesToScroll: 1,
        slidesToShow: 1,
        infinite: true,
        autoplay: false,
    });
    bulmaCarousel.attach('#results-carousel44', {
        slidesToScroll: 1,
        slidesToShow: 2,
        infinite: false,
        autoplay: false,
    });
</script>

<script>
    document.getElementById('gifImage3').src = 'content/gifs/Item.gif';
    document.getElementById('gifImage1').src = 'content/gifs/s1.gif';
    document.getElementById('gifImage2').src = 'content/gifs/s2.gif';

    // 图片资源路径
    const images = [
        'content/teaser/t3.gif',
        'content/teaser/t4.gif',
        'content/teaser/t1.gif',
        'content/teaser/t2.gif'
    ];

    // 获取要插入图片的div
    const group1 = document.getElementById('group1');
    const group2 = document.getElementById('group2');

    // 创建并插入前两张图片
    for (let i = 0; i < 2; i++) {
        const img = document.createElement('img');
        img.src = images[i];
        img.loading = 'lazy';
        img.alt = '图片' + (i + 1);
        group1.appendChild(img);
    }

    // 创建并插入后两张图片
    for (let i = 2; i < images.length; i++) {
        const img = document.createElement('img');
        img.src = images[i];
        img.loading = 'lazy';
        img.alt = '图片' + (i + 1);
        group2.appendChild(img);
    }

</script>

</body>


</html>
